# Nexus MCP RAG Server - Implementation Plan

## Technology Stack

| Component | Technology | Purpose |
|-----------|------------|---------|
| **Embeddings** | `sentence-transformers` (bge-base-en-v1.5) | Text embeddings |
| **Vector DB** | `qdrant-client` (embedded) | Vector storage |
| **Keyword Search** | `rank-bm25` | BM25 lexical search |
| **Reranking** | `sentence-transformers` | Cross-encoder |
| **Metadata** | SQLite | Source tracking |
| **MCP** | `mcp` SDK | Tool protocol |
| **Evaluation** | **RAGAS** | RAG quality metrics |
| **Test Client** | **MLX-LM** | Local LLM for testing MCP |

---

## RAGAS Evaluation Framework

Using RAGAS (`pip install ragas`) for comprehensive RAG evaluation:

| Metric | Layer | Target | Description |
|--------|-------|--------|-------------|
| **Context Precision** | Retrieval | ≥0.80 | Relevant chunks ranked higher |
| **Context Recall** | Retrieval | ≥0.85 | All needed info retrieved |
| **Faithfulness** | Generation | ≥0.90 | Answer grounded in context |
| **Answer Relevancy** | Generation | ≥0.85 | Answer addresses question |

**RAGAS requires an LLM for evaluation** → We'll use MLX-LM as the judge model.

---

## Sprint Plan

### Sprint 1: Project Setup (Days 1-2)
- [ ] venv + dependencies (sentence-transformers, qdrant, ragas, mcp)
- [ ] Directory structure per AGENTS.md
- [ ] Config system
- **✓ Test:** Imports work, RAGAS loads

### Sprint 2: Storage Layer (Days 3-4)
- [ ] Pydantic models (Document, Chunk, Source)
- [ ] SQLite metadata + Qdrant vectors
- **✓ Test:** Storage CRUD passes

### Sprint 3: Embedding & Ingestion (Days 5-7)
- [ ] sentence-transformers embedder
- [ ] Markdown loader + chunker
- **✓ Test:** Embeddings generated correctly

### Sprint 4: Vector Search (Days 8-9)
- [ ] Query embedding + vector search
- [ ] Metadata filtering
- **✓ Test:** Search returns results

### Sprint 5: Hybrid Search + Reranking (Days 10-12)
- [ ] BM25 + RRF fusion
- [ ] Cross-encoder reranking
- **✓ Test:** Hybrid > vector-only

### Sprint 6: MCP Server (Days 13-15)
- [ ] MCP server + tools
- [ ] **MLX-LM integration** as test client
- **✓ Test:** Tools callable from LLM

### Sprint 7: CLI (Days 16-18)
- [ ] Typer CLI (init, index, serve)
- [ ] File watching
- **✓ Test:** CLI commands work

### Sprint 8: RAGAS Evaluation (Days 19-21)
- [ ] Create 50+ query test set with ground truth
- [ ] Run RAGAS evaluation with MLX-LM as judge
- [ ] Meet quality thresholds
- [ ] Documentation
- **✓ Test:** All RAGAS metrics pass thresholds

---

## RAGAS Integration Details

```python
# Sprint 8 evaluation code pattern
from ragas import evaluate
from ragas.metrics import context_precision, context_recall, faithfulness, answer_relevancy

# Prepare test dataset
dataset = {
    "question": [...],
    "answer": [...],        # Generated by MLX-LM
    "contexts": [...],      # Retrieved by Nexus
    "ground_truth": [...]   # Manual labels
}

# Run evaluation (uses MLX-LM as judge via OpenAI-compatible API)
result = evaluate(dataset, metrics=[
    context_precision,
    context_recall,
    faithfulness,
    answer_relevancy
])
```

---

## MLX-LM Usage

1. **As Test Client (Sprint 6):** Call MCP tools during development
2. **As RAGAS Judge (Sprint 8):** Evaluate retrieval/generation quality

```bash
# Start MLX-LM server
mlx_lm.server --model mlx-community/Qwen2.5-7B-Instruct-4bit --port 8080
```

---

## Ready to proceed with Sprint 1?
